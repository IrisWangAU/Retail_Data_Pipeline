{"cells":[{"cell_type":"markdown","id":"ef36f535-4bdc-4e2b-a22a-179372324b26","metadata":{},"source":["![walmartecomm](walmartecomm.jpg)\n","\n","Walmart is the biggest retail store in the United States. Just like others, they have been expanding their e-commerce part of the business. By the end of 2022, e-commerce represented a roaring $80 billion in sales, which is 13% of total sales of Walmart. One of the main factors that affects their sales is public holidays, like the Super Bowl, Labour Day, Thanksgiving, and Christmas. \n","\n","In this project, a data pipeline for the analysis of supply and demand around the holidays, along with conducting a preliminary analysis of the data. There are two data sources: grocery sales and complementary data. You have been provided with the `grocery_sales` table with the following features:\n","\n","# `grocery_sales`\n","- `\"index\"` - unique ID of the row\n","- `\"Store_ID\"` - the store number\n","- `\"Date\"` - the week of sales\n","- `\"Weekly_Sales\"` - sales for the given store\n","\n","Also, the `extra_data.parquet` file that contains complementary data:\n","\n","# `extra_data.parquet`\n","- `\"IsHoliday\"` - Whether the week contains a public holiday - 1 if yes, 0 if no.\n","- `\"Temperature\"` - Temperature on the day of sale\n","- `\"Fuel_Price\"` - Cost of fuel in the region\n","- `\"CPI\"` â€“ Prevailing consumer price index\n","- `\"Unemployment\"` - The prevailing unemployment rate\n","- `\"MarkDown1\"`, `\"MarkDown2\"`, `\"MarkDown3\"`, `\"MarkDown4\"` - number of promotional markdowns\n","- `\"Dept\"` - Department Number in each store\n","- `\"Size\"` - size of the store\n","- `\"Type\"` - type of the store (depends on `Size` column)\n","\n","The two files need to be merged and manipulated. The transformed DataFrame can then be stored as the `clean_data` variable containing the following columns:\n","- `\"Store_ID\"`\n","- `\"Month\"`\n","- `\"Dept\"`\n","- `\"IsHoliday\"`\n","- `\"Weekly_Sales\"`\n","- `\"CPI\"`\n","- \"`\"Unemployment\"`\"\n","\n","After merging and cleaning the data, the monthly sales of Walmart are stored  as the `agg_data` variable that look like:\n","\n","|  Month | Weekly_Sales  | \n","|---|---|\n","| 01  |  33174.178494 |\n","|  02 |  34333.326579 |\n","|  ... | ...  |  \n","\n","Finally, the `clean_data` and `agg_data` files are stored as csv files."]},{"cell_type":"code","execution_count":8,"id":"c0d64ff1-a4ca-4a82-a8b4-e210244dedc1","metadata":{},"outputs":[],"source":["# load libraries\n","import pandas as pd\n","import os"]},{"cell_type":"code","execution_count":9,"id":"12534569","metadata":{},"outputs":[],"source":["# functions to extract data from sources\n","\n","def extract_parq(parquet_file_path, col):\n","    parquet_df = pd.read_parquet(parquet_file_path, engine='fastparquet')\n","    return parquet_df[col]\n","\n","def extract_csv(csv_file_path):\n","    csv_df = pd.read_csv(csv_file_path)\n","    return csv_df"]},{"cell_type":"code","execution_count":10,"id":"acc1dce0","metadata":{},"outputs":[],"source":["# cleaning data into desired format\n","\n","def transform(merged_df):\n","    # Fill in the NAs in Date, CPI, Unemployment with the last non-null values\n","    merged_df['Date'] = merged_df['Date'].fillna(method = 'ffill')\n","    merged_df['CPI'] = merged_df['CPI'].fillna(method = 'ffill')\n","    merged_df['Unemployment'] = merged_df['Unemployment'].fillna(method = 'ffill')\n","\n","    # drop any rows with no recording of weekly sales\n","    merged_df.dropna(subset='Weekly_Sales', inplace=True)\n","\n","    # get the month form the dates\n","    merged_df['Date'] = pd.to_datetime(merged_df['Date'], format='%Y-%m-%d')\n","    merged_df['Month'] = merged_df['Date'].dt.strftime('%m')\n","\n","    # keep the rows where the weekly sales are over $10,000\n","    merged_df = merged_df.loc[merged_df[\"Weekly_Sales\"] > 10000, :]\n","\n","    return merged_df[['Store_ID','Month','Dept','IsHoliday','Weekly_Sales','CPI','Unemployment']]"]},{"cell_type":"code","execution_count":11,"id":"29c02e3c","metadata":{},"outputs":[],"source":["# get the average weekly sales by month\n","def avg_monthly_sales(clean_data):\n","    # group the weekly sales by month\n","    groups = clean_data.groupby(by=['Month'])['Weekly_Sales'].sum()\n","    # create a dataframe for the averge weekly sales by month\n","    agg_data = pd.DataFrame(groups)\n","    # rename the column names\n","    agg_data = agg_data.reset_index()\n","    agg_data = agg_data.rename(columns={'Weekly_Sales':'Avg_Sales'})\n","\n","    return agg_data"]},{"cell_type":"code","execution_count":12,"id":"3b38cd60","metadata":{},"outputs":[],"source":["# save the data to csv files\n","def load_csv(clean_data, csv_path):\n","    clean_data.to_csv(csv_path, index=False)"]},{"cell_type":"code","execution_count":13,"id":"1b5e8c6b","metadata":{},"outputs":[],"source":["# to validate the existence of clean_data and agg_data file\n","def validation(file_path):\n","    if os.path.isfile(file_path):\n","        print('CSV file exist in the current directory')\n","    else:\n","        print(\"The CSV file do not exist in the current working directory\")"]},{"cell_type":"code","execution_count":14,"id":"eb231b82","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO:Successfully extracted, transformed and loaded data.\n"]},{"name":"stdout","output_type":"stream","text":["CSV file exist in the current directory\n","CSV file exist in the current directory\n"]}],"source":["import logging\n","\n","logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.DEBUG)\n","\n","try:\n","    # Extract, Transform, Aggregate and Load Data\n","    grocery_sales_df = extract_csv('data/grocery_sales.csv')\n","    extra_data_df = extract_parq('data/extra_data.parquet',  ['index','IsHoliday', 'CPI','Unemployment'])\n","\n","    # merge two tables together\n","    merged_df = grocery_sales_df.merge(extra_data_df, left_on='index', right_on='index')\n","\n","    # transform the data to get clean data\n","    clean_data = transform(merged_df)\n","\n","    # gather average weekly sales by month\n","    agg_data = avg_monthly_sales(clean_data)\n","    \n","    # give the file path for loading\n","    clean_data_path = 'data/clean_data.csv'\n","    agg_data_path = 'data/agg_data.csv'\n","\n","    # loading the cleaned data and aggregated data\n","    load_csv(clean_data,clean_data_path)\n","    load_csv(agg_data,agg_data_path)\n","\n","    # validate the existence of clean_data and agg_data file\n","    validation(clean_data_path)\n","    validation(agg_data_path)\n","\n","    logging.info(\"Successfully extracted, transformed and loaded data.\") # Log success message\n","\n","# Handle exceptions, log message\n","except Exception as e:\n","    logging.error(f\"Pipeline failed with error: {e}\")\n","    "]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"}},"nbformat":4,"nbformat_minor":5}
