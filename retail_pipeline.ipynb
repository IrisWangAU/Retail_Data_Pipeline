{"cells":[{"cell_type":"code","execution_count":1,"id":"c0d64ff1-a4ca-4a82-a8b4-e210244dedc1","metadata":{},"outputs":[],"source":["# load libraries\n","import pandas as pd\n","import os"]},{"cell_type":"code","execution_count":2,"id":"12534569","metadata":{},"outputs":[],"source":["# functions to extract data from sources\n","\n","def extract_parq(parquet_file_path, col):\n","    parquet_df = pd.read_parquet(parquet_file_path, engine='fastparquet')\n","    return parquet_df[col]\n","\n","def extract_csv(csv_file_path):\n","    csv_df = pd.read_csv(csv_file_path)\n","    return csv_df"]},{"cell_type":"code","execution_count":3,"id":"acc1dce0","metadata":{},"outputs":[],"source":["# cleaning data into desired format\n","\n","def transform(merged_df):\n","    # Fill in the NAs in Date, CPI, Unemployment with the last non-null values\n","    merged_df['Date'] = merged_df['Date'].fillna(method = 'ffill')\n","    merged_df['CPI'] = merged_df['CPI'].fillna(method = 'ffill')\n","    merged_df['Unemployment'] = merged_df['Unemployment'].fillna(method = 'ffill')\n","\n","    # drop any rows with no recording of weekly sales\n","    merged_df.dropna(subset='Weekly_Sales', inplace=True)\n","\n","    # get the month form the dates\n","    merged_df['Date'] = pd.to_datetime(merged_df['Date'], format='%Y-%m-%d')\n","    merged_df['Month'] = merged_df['Date'].dt.strftime('%m')\n","\n","    # keep the rows where the weekly sales are over $10,000\n","    merged_df = merged_df.loc[merged_df[\"Weekly_Sales\"] > 10000, :]\n","\n","    return merged_df[['Store_ID','Month','Dept','IsHoliday','Weekly_Sales','CPI','Unemployment']]"]},{"cell_type":"code","execution_count":15,"id":"29c02e3c","metadata":{},"outputs":[],"source":["# get the average weekly sales by month\n","def avg_monthly_sales(clean_data):\n","    # group the weekly sales by month\n","    groups = clean_data.groupby(by=['Month'])['Weekly_Sales'].mean()\n","    # create a dataframe for the averge weekly sales by month\n","    agg_data = pd.DataFrame(groups)\n","    # rename the column names\n","    agg_data = agg_data.reset_index()\n","    agg_data = agg_data.rename(columns={'Weekly_Sales':'Avg_Weekly_Sales'})\n","\n","    return agg_data"]},{"cell_type":"code","execution_count":16,"id":"3b38cd60","metadata":{},"outputs":[],"source":["# save the data to csv files\n","def load_csv(clean_data, csv_path):\n","    clean_data.to_csv(csv_path, index=False)"]},{"cell_type":"code","execution_count":17,"id":"1b5e8c6b","metadata":{},"outputs":[],"source":["# to validate the existence of clean_data and agg_data file\n","def validation(file_path):\n","    if os.path.isfile(file_path):\n","        print('CSV file exist in the current directory')\n","    else:\n","        print(\"The CSV file do not exist in the current working directory\")"]},{"cell_type":"code","execution_count":18,"id":"eb231b82","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO:Successfully extracted, transformed and loaded data.\n"]},{"name":"stdout","output_type":"stream","text":["CSV file exist in the current directory\n","CSV file exist in the current directory\n"]}],"source":["import logging\n","\n","logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.DEBUG)\n","\n","try:\n","    # Extract, Transform, Aggregate and Load Data\n","    grocery_sales_df = extract_csv('data/grocery_sales.csv')\n","    extra_data_df = extract_parq('data/extra_data.parquet',  ['index','IsHoliday', 'CPI','Unemployment'])\n","\n","    # merge two tables together\n","    merged_df = grocery_sales_df.merge(extra_data_df, left_on='index', right_on='index')\n","\n","    # transform the data to get clean data\n","    clean_data = transform(merged_df)\n","\n","    # gather average weekly sales by month\n","    agg_data = avg_monthly_sales(clean_data)\n","    \n","    # give the file path for loading\n","    clean_data_path = 'data/clean_data.csv'\n","    agg_data_path = 'data/agg_data.csv'\n","\n","    # loading the cleaned data and aggregated data\n","    load_csv(clean_data,clean_data_path)\n","    load_csv(agg_data,agg_data_path)\n","\n","    # validate the existence of clean_data and agg_data file\n","    validation(clean_data_path)\n","    validation(agg_data_path)\n","\n","    logging.info(\"Successfully extracted, transformed and loaded data.\") # Log success message\n","\n","# Handle exceptions, log message\n","except Exception as e:\n","    logging.error(f\"Pipeline failed with error: {e}\")\n","    "]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"}},"nbformat":4,"nbformat_minor":5}
